# backend/services/ai_service.py
"""
AI æœåŠ¡æ¨¡å— - æ”¯æŒæ¨¡å‹ç®¡ç†ä¸é™çº§
"""

import logging
import httpx
import json
import hashlib
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any

try:
    from ..config import get_settings
    # from .vector_service import get_vector_service
    from ..utils.cache import get_cache_manager
except (ImportError, ValueError):
    # Fallback for direct script execution or different import paths
    from config import get_settings
    # from services.vector_service import get_vector_service
    from utils.cache import get_cache_manager
from ..database import get_db

logger = logging.getLogger(__name__)


class AIService:
    """AI æœåŠ¡ç±»"""
    
    # æ¨èæ¨¡å‹ä¼˜å…ˆçº§ï¼ˆæ ¹æ®ç”¨æˆ·å¼ºåˆ¶è¦æ±‚çš„ DeepSeek-V3 ç³»åˆ—ï¼‰
    RECOMMENDED_MODELS = [
        'DeepSeek-V3',
        'DeepSeek-V3-0324',
        'deepseek-v3-1-250821',
        'deepseek-v3-1-terminus',
        'deepseek-v3-2-251201',
        'deepseek-v3.2-speciale',
        'gpt-oss-120b'
    ]
    
    def __init__(self):
        self.settings = get_settings()
        self.db = get_db()
        
        self.api_key = self.settings.AI_API_KEY
        self.base_url = self.settings.AI_BASE_URL.rstrip('/')
        # è‡ªåŠ¨çº æ­£ï¼šå¦‚æœç”¨æˆ·æä¾›çš„ URL åŒ…å«äº† /chat/completionsï¼Œæˆ‘ä»¬éœ€è¦å»æ‰å®ƒï¼Œ
        # å› ä¸ºåç»­ä»£ç ä¼šè‡ªåŠ¨æ‹¼æ¥è¿™ä¸ªåç¼€ï¼Œæˆ–è€…åœ¨è·å–æ¨¡å‹åˆ—è¡¨æ—¶éœ€è¦æ‹¼æ¥ /models
        if self.base_url.endswith('/chat/completions'):
            self.base_url = self.base_url.replace('/chat/completions', '').rstrip('/')
            
        self.current_model = self.settings.AI_MODEL
        self.fallback_models = self.settings.AI_FALLBACK_MODELS
        self.timeout = self.settings.AI_TIMEOUT
        
        # ç¼“å­˜
        self._available_models: List[str] = []
        self._models_cache_time: Optional[datetime] = None
        self._models_cache_ttl = 3600  # 1å°æ—¶
        
        logger.info(f"AIæœåŠ¡åˆå§‹åŒ–: model={self.current_model}, base_url={self.base_url}")
    
    async def fetch_available_models(self, force_refresh: bool = False) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        # æ£€æŸ¥ç¼“å­˜
        if not force_refresh and self._available_models and self._models_cache_time:
            if (datetime.now() - self._models_cache_time).seconds < self._models_cache_ttl:
                return self._available_models
        
        try:
            async with httpx.AsyncClient(timeout=30) as client:
                response = await client.get(
                    f"{self.base_url}/models",
                    headers={"Authorization": f"Bearer {self.api_key}"}
                )
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    # è§£ææ¨¡å‹åˆ—è¡¨
                    if isinstance(data, dict) and 'data' in data:
                        for item in data['data']:
                            if isinstance(item, dict) and 'id' in item:
                                models.append(item['id'])
                            elif isinstance(item, str):
                                models.append(item)
                    elif isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and 'id' in item:
                                models.append(item['id'])
                            elif isinstance(item, str):
                                models.append(item)
                    
                    self._available_models = sorted(models)
                    self._models_cache_time = datetime.now()
                    logger.info(f"è·å–åˆ° {len(models)} ä¸ªå¯ç”¨æ¨¡å‹")
                    return self._available_models
                else:
                    # å¦‚æœè¿”å› 404ï¼Œè¯´æ˜è¯¥æä¾›å•†å¯èƒ½ä¸æ”¯æŒé€šç”¨æ¨¡å‹æŸ¥è¯¢æ¥å£ï¼Œè¿™åœ¨å¾ˆå¤šä¸­è½¬ API ä¸­å¾ˆå¸¸è§
                    if response.status_code == 404:
                        logger.debug(f"æ¨¡å‹åˆ—è¡¨æ¥å£ä¸å¯ç”¨ (404): {self.base_url}/models")
                    else:
                        logger.warning(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {response.status_code}")
                    
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¼‚å¸¸: {e}")
        
        return self._available_models or []

    # generate_deep_analysis å·²åˆ é™¤
    
    def get_recommended_models(self) -> List[str]:
        """è·å–æ¨èæ¨¡å‹åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰"""
        if not self._available_models:
            return self.RECOMMENDED_MODELS
        
        # è¿”å›å¯ç”¨ä¸”æ¨èçš„æ¨¡å‹
        recommended = []
        for model in self.RECOMMENDED_MODELS:
            if model in self._available_models:
                recommended.append(model)
        
        return recommended if recommended else self._available_models[:10]
    
    def get_model_info(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            'current_model': self.current_model,
            'fallback_models': self.fallback_models,
            'available_count': len(self._available_models),
            'available_models': self._available_models[:50],
            'recommended_models': self.get_recommended_models(),
            'api_configured': bool(self.api_key),
            'base_url': self.base_url
        }
    
    def _get_models_to_try(self) -> List[str]:
        """è·å–è¦å°è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼ˆæ™ºèƒ½é€‰æ‹©ï¼šä¼˜å…ˆå¯ç”¨ä¸”æ¨èçš„æ¨¡å‹ï¼‰"""
        models = []
        
        # å¦‚æœæœ‰å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼Œä½¿ç”¨å®ƒæ¥è¿‡æ»¤
        if self._available_models:
            # 1. å…ˆåŠ å…¥ä¸»æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.current_model in self._available_models:
                models.append(self.current_model)
            
            # 2. åŠ å…¥æ¨èæ¨¡å‹ä¸­å¯ç”¨çš„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            for m in self.RECOMMENDED_MODELS:
                if m in self._available_models and m not in models:
                    models.append(m)
            
            # 3. åŠ å…¥fallbackæ¨¡å‹ä¸­å¯ç”¨çš„
            for m in self.fallback_models:
                if m in self._available_models and m not in models:
                    models.append(m)
            
            # 4. å¦‚æœä¸Šè¿°éƒ½æ²¡æœ‰ï¼Œä»å¯ç”¨æ¨¡å‹ä¸­é€‰æ‹©å‰10ä¸ª
            if not models:
                models = self._available_models[:10]
        else:
            # æ²¡æœ‰å¯ç”¨æ¨¡å‹åˆ—è¡¨æ—¶ï¼Œä½¿ç”¨é»˜è®¤é¡ºåº
            models = [self.current_model]
            for m in self.fallback_models:
                if m not in models:
                    models.append(m)
            # æ·»åŠ æ¨èæ¨¡å‹ä½œä¸ºé¢å¤–å¤‡é€‰
            for m in self.RECOMMENDED_MODELS:
                if m not in models:
                    models.append(m)
        
        logger.debug(f"å°†å°è¯•çš„æ¨¡å‹åˆ—è¡¨: {models[:5]}...")
        return models
    
    async def _call_ai(
        self, 
        system_prompt: str, 
        user_prompt: str, 
        max_tokens: int = 2000,
        timeout: int = None
    ) -> Optional[str]:
        """è°ƒç”¨ AI APIï¼ˆæ”¯æŒæ™ºèƒ½æ¨¡å‹é™çº§ï¼‰"""
        # ä½¿ç”¨è‡ªå®šä¹‰è¶…æ—¶æˆ–é»˜è®¤è¶…æ—¶
        request_timeout = timeout if timeout is not None else self.timeout
        # å…ˆå°è¯•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        if not self._available_models:
            await self.fetch_available_models()
        
        models_to_try = self._get_models_to_try()
        errors = []
        
        for model in models_to_try:
            try:
                async with httpx.AsyncClient(timeout=request_timeout) as client:
                    response = await client.post(
                        f"{self.base_url}/chat/completions",
                        headers={
                            "Authorization": f"Bearer {self.api_key}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "model": model,
                            "messages": [
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": user_prompt}
                            ],
                            "max_tokens": max_tokens,
                            "temperature": 0.3
                        }
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        content = data['choices'][0]['message']['content']
                        logger.info(f"AI è°ƒç”¨æˆåŠŸ: model={model}")
                        self.current_model = model # æ›´æ–°ä¸ºä¸»é€‰æ¨¡å‹
                        return content
                    else:
                        error_msg = response.json().get('error', {}).get('message', 'æœªçŸ¥é”™è¯¯')
                        errors.append(f"{model}: {error_msg}")
                        logger.warning(f"AI æ¨¡å‹ {model} è°ƒç”¨å¤±è´¥: {response.status_code} - {error_msg}")
            except Exception as e:
                errors.append(f"{model}: {str(e)}")
                logger.error(f"AI æ¨¡å‹ {model} å¼‚å¸¸: {e}")
        
        
        logger.error(f"æ‰€æœ‰æ¨¡å‹å°è¯•å‡å¤±è´¥: {'; '.join(errors)}")
        return None

    def _generate_metrics_hash(self, metrics: Any) -> str:
        """ç”ŸæˆæŒ‡æ ‡çš„ MD5 å“ˆå¸Œ"""
        if not metrics:
            return "empty"
        # ç¡®ä¿å­—å…¸é”®æœ‰åº
        metrics_str = json.dumps(metrics, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(metrics_str.encode()).hexdigest()
    
    async def ask_ai(self, prompt: str, system_prompt: str = None, max_tokens: int = 2000, timeout: int = None) -> Dict[str, Any]:
        """é€šç”¨AIé—®ç­”æ¥å£ï¼ˆä¾›å…¶ä»–æ¨¡å—è°ƒç”¨ï¼‰"""
        if system_prompt is None:
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œæä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æå’Œå»ºè®®ã€‚"
        
        content = await self._call_ai(system_prompt, prompt, max_tokens, timeout=timeout)
        
        if content:
            return {
                'success': True,
                'content': content
            }
        else:
            return {
                'success': False,
                'error': 'æ‰€æœ‰AIæ¨¡å‹éƒ½ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•'
            }
    
    async def generate_fund_analysis(
        self, 
        code: str, 
        metrics: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”ŸæˆåŸºé‡‘åˆ†æ"""
        # ç»Ÿä¸€ç¼“å­˜é”®ç­–ç•¥: åŠŸèƒ½:ä»£ç :æŒ‡æ ‡å“ˆå¸Œ
        metrics_hash = self._generate_metrics_hash(metrics)
        cache_key = f"fund_analysis:{code}:{metrics_hash}"
        cached = self.db.get_ai_cache(cache_key)
        if cached:
            return {
                'success': True,
                'content': cached,
                'source': 'cache'
            }
        
        # æ„å»ºæç¤ºè¯
        system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŸºé‡‘åˆ†æå¸ˆï¼Œæ“…é•¿è§£è¯»åŸºé‡‘çš„é‡åŒ–æŒ‡æ ‡å¹¶ç»™å‡ºæŠ•èµ„å»ºè®®ã€‚
è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€åˆ†æåŸºé‡‘ï¼ŒåŒ…æ‹¬ï¼š
1. æ€»ä½“è¯„ä»·ï¼ˆ1-2å¥è¯ï¼‰
2. ä¸»è¦ä¼˜åŠ¿ï¼ˆ2-3ç‚¹ï¼‰
3. æ½œåœ¨é£é™©ï¼ˆ1-2ç‚¹ï¼‰
4. æŠ•èµ„å»ºè®®ï¼ˆé€‚åˆä»€ä¹ˆç±»å‹çš„æŠ•èµ„è€…ï¼‰

ä½¿ç”¨ Markdown æ ¼å¼ï¼Œä¿æŒç®€æ´ï¼Œæ€»å­—æ•°æ§åˆ¶åœ¨ 300 å­—ä»¥å†…ã€‚"""
        
        user_prompt = self._build_fund_prompt(code, metrics)
        
        # è°ƒç”¨ AI
        content = await self._call_ai(system_prompt, user_prompt, max_tokens=1000)
        
        if content:
            # ä¿å­˜ç¼“å­˜
            self.db.set_ai_cache(cache_key, content, self.current_model, ttl_hours=24)
            return {
                'success': True,
                'content': content,
                'source': 'ai',
                'model': self.current_model
            }
        
        # é™çº§ä¸ºè§„åˆ™åŒ–åˆ†æ
        fallback_content = self._generate_fallback_analysis(code, metrics)
        return {
            'success': True,
            'content': fallback_content,
            'source': 'fallback'
        }
    
    def _build_fund_prompt(self, code: str, metrics: Dict) -> str:
        """æ„å»ºåŸºé‡‘åˆ†ææç¤ºè¯"""
        name = metrics.get('name', code)
        
        prompt = f"""è¯·åˆ†æä»¥ä¸‹åŸºé‡‘ï¼š

**åŸºé‡‘åç§°**: {name}
**åŸºé‡‘ä»£ç **: {code}
**æ•°æ®æ—¥æœŸ**: {metrics.get('nav_date', 'æœªçŸ¥')}

**æ ¸å¿ƒæŒ‡æ ‡**:
- Alpha: {metrics.get('alpha', 'N/A')}%
- Beta: {metrics.get('beta', 'N/A')}
- å¤æ™®æ¯”ç‡: {metrics.get('sharpe', 'N/A')}
- å¹´åŒ–æ”¶ç›Š: {metrics.get('annual_return', 'N/A')}%
- å¹´åŒ–æ³¢åŠ¨ç‡: {metrics.get('volatility', 'N/A')}%
- æœ€å¤§å›æ’¤: {metrics.get('max_drawdown', 'N/A')}%
- å½“å‰å›æ’¤: {metrics.get('current_drawdown', 'N/A')}%
- èƒœç‡: {metrics.get('win_rate', 'N/A')}%
- ç›ˆäºæ¯”: {metrics.get('profit_loss_ratio', 'N/A')}

**æ”¶ç›Šè¡¨ç°**:
- è¿‘1å‘¨: {metrics.get('return_1w', 'N/A')}%
- è¿‘1æœˆ: {metrics.get('return_1m', 'N/A')}%
- è¿‘3æœˆ: {metrics.get('return_3m', 'N/A')}%
- è¿‘6æœˆ: {metrics.get('return_6m', 'N/A')}%
- è¿‘1å¹´: {metrics.get('return_1y', 'N/A')}%

**ç»¼åˆè¯„åˆ†**: {metrics.get('score', 'N/A')}/100
"""
        return prompt
    
    def _generate_fallback_analysis(self, code: str, metrics: Dict) -> str:
        """ç”Ÿæˆè§„åˆ™åŒ–é™çº§åˆ†æ"""
        name = metrics.get('name', code)
        score = metrics.get('score', 0)
        alpha = metrics.get('alpha', 0)
        sharpe = metrics.get('sharpe', 0)
        max_dd = metrics.get('max_drawdown', 0)
        annual_return = metrics.get('annual_return', 0)
        
        # è¯„çº§
        if score >= 80:
            rating = "ä¼˜ç§€"
            rating_desc = "è¯¥åŸºé‡‘ç»¼åˆè¡¨ç°çªå‡º"
        elif score >= 60:
            rating = "è‰¯å¥½"
            rating_desc = "è¯¥åŸºé‡‘ç»¼åˆè¡¨ç°è¾ƒå¥½"
        elif score >= 40:
            rating = "ä¸€èˆ¬"
            rating_desc = "è¯¥åŸºé‡‘è¡¨ç°ä¸­è§„ä¸­çŸ©"
        else:
            rating = "è¾ƒå¼±"
            rating_desc = "è¯¥åŸºé‡‘è¡¨ç°æ¬ ä½³"
        
        # ä¼˜åŠ¿åˆ†æ
        advantages = []
        if alpha > 5:
            advantages.append(f"è¶…é¢æ”¶ç›Šèƒ½åŠ›å¼ºï¼ŒAlpha è¾¾ {alpha}%")
        if sharpe > 1.5:
            advantages.append(f"é£é™©è°ƒæ•´åæ”¶ç›Šä¼˜ç§€ï¼Œå¤æ™®æ¯”ç‡ {sharpe}")
        if max_dd < 15:
            advantages.append(f"å›æ’¤æ§åˆ¶è‰¯å¥½ï¼Œæœ€å¤§å›æ’¤ä»… {max_dd}%")
        if annual_return > 20:
            advantages.append(f"å¹´åŒ–æ”¶ç›Šå¯è§‚ï¼Œè¾¾ {annual_return}%")
        
        if not advantages:
            advantages.append("å„é¡¹æŒ‡æ ‡è¡¨ç°å‡è¡¡")
        
        # é£é™©æç¤º
        risks = []
        if max_dd > 30:
            risks.append(f"å›æ’¤è¾ƒå¤§ï¼ˆ{max_dd}%ï¼‰ï¼Œéœ€å…³æ³¨é£é™©æ‰¿å—èƒ½åŠ›")
        if metrics.get('volatility', 0) > 25:
            risks.append(f"æ³¢åŠ¨è¾ƒé«˜ï¼Œé€‚åˆé£é™©åå¥½è¾ƒé«˜çš„æŠ•èµ„è€…")
        if alpha < 0:
            risks.append("Alpha ä¸ºè´Ÿï¼Œè·‘è¾“åŸºå‡†")
        
        if not risks:
            risks.append("æš‚æ— æ˜æ˜¾é£é™©ç‚¹ï¼Œä½†æŠ•èµ„éœ€è°¨æ…")
        
        # ç»„è£…åˆ†æ
        analysis = f"""## {name} åˆ†ææŠ¥å‘Š

### æ€»ä½“è¯„ä»·
{rating_desc}ï¼Œç»¼åˆè¯„åˆ† **{score}** åˆ†ï¼ˆ{rating}ï¼‰ã€‚

### ä¸»è¦ä¼˜åŠ¿
"""
        for adv in advantages[:3]:
            analysis += f"- {adv}\n"
        
        analysis += "\n### æ½œåœ¨é£é™©\n"
        for risk in risks[:2]:
            analysis += f"- {risk}\n"
        
        analysis += f"""
### æŠ•èµ„å»ºè®®
"""
        if score >= 70 and max_dd < 20:
            analysis += "é€‚åˆè¿½æ±‚ç¨³å¥æ”¶ç›Šçš„é•¿æœŸæŠ•èµ„è€…ï¼Œå¯è€ƒè™‘ä½œä¸ºæ ¸å¿ƒé…ç½®ã€‚"
        elif score >= 60:
            analysis += "é€‚åˆæœ‰ä¸€å®šé£é™©æ‰¿å—èƒ½åŠ›çš„æŠ•èµ„è€…ï¼Œå»ºè®®ä½œä¸ºå«æ˜Ÿé…ç½®ã€‚"
        else:
            analysis += "å»ºè®®è°¨æ…è€ƒè™‘ï¼Œæˆ–ç­‰å¾…æ›´å¥½çš„å…¥åœºæ—¶æœºã€‚"
        
        analysis += "\n\n*ï¼ˆæ­¤åˆ†æç”±è§„åˆ™å¼•æ“ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒï¼‰*"
        
        return analysis
    
    async def generate_portfolio_diagnosis(
        self, 
        portfolio_data: List[Dict],
        stats: Dict = None
    ) -> Dict[str, Any]:
        """ç”ŸæˆæŒä»“è¯Šæ–­æŠ¥å‘Š (å¸¦ç¼“å­˜)"""
        if not portfolio_data:
            return {
                'success': False,
                'error': 'æ— æŒä»“æ•°æ®'
            }
        
        # å¦‚æœæ²¡æœ‰ä¼  statsï¼Œç®€å•çš„è®¡ç®—ä¸€äº›åŸºç¡€ç»Ÿè®¡
        if stats is None:
            total_shares = sum(p.get('shares', 0) for p in portfolio_data)
            stats = {
                'position_count': len(portfolio_data),
                'total_cost': sum(p.get('shares', 0) * p.get('cost_price', 0) for p in portfolio_data),
                'total_value': 0, # è¿™é‡Œæ²¡æœ‰æœ€æ–°å‡€å€¼æ— æ³•å‡†ç¡®è®¡ç®—ï¼Œæç¤ºé‡Œä¼šè¯´æ˜
                'category_distribution': {},
                'theme_distribution': {}
            }
        
        try:
            # 0. æ£€æŸ¥ç¼“å­˜
            metrics_hash = self._generate_metrics_hash({
                "portfolio": [{"c": p['fund_code'], "s": p.get('shares', 0)} for p in portfolio_data],
                "stats": stats
            })
            cache_key = f"portfolio_diagnosis:{metrics_hash}"
            cache = get_cache_manager()
            cached = cache.get(cache_key)
            if cached:
                logger.info(f"æŒä»“è¯Šæ–­å‘½ä¸­ç¼“å­˜: {cache_key}")
                return cached

            # 1. æ„å»ºæç¤ºè¯
            prompt = self._build_portfolio_prompt(portfolio_data, stats)
            
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢å¯Œç®¡ç†é¡¾é—®å’Œå®šæŠ•ä¸“å®¶ã€‚
è¯·æ ¹æ®ç”¨æˆ·çš„åŸºé‡‘æŒä»“æ•°æ®ï¼Œä»ä»¥ä¸‹ç»´åº¦è¿›è¡Œæ·±åº¦è¯Šæ–­ï¼š
1. èµ„äº§é…ç½®ï¼šè‚¡å€ºæ¯”ä¾‹æ˜¯å¦åˆç†ï¼Œåˆ†æ•£åº¦å¦‚ä½•ã€‚
2. é£æ ¼ç©¿é€ï¼šæ˜¯å¦å­˜åœ¨ç‰¹å®šè¡Œä¸šæˆ–é£æ ¼çš„è¿‡åº¦æš´éœ²ã€‚
3. é£é™©æç¤ºï¼šå½“å‰ç»„åˆæœ€å¤§çš„æ½œåœ¨å›æ’¤é£é™©ã€‚
4. è°ƒä»“å»ºè®®ï¼šç»™å‡ºå…·ä½“ã€å¯æ‰§è¡Œçš„æ“ä½œå»ºè®®ã€‚
è¯·ä½¿ç”¨ Markdown æ ¼å¼ï¼Œè¯­æ°”ä¸“ä¸šä¸”å¯Œæœ‰åŒç†å¿ƒã€‚"""

            # 2. è°ƒç”¨ AI
            content = await self.ask_ai(prompt, system_prompt=system_prompt)
            result = {
                'success': True,
                'content': content
            }
            # ç¼“å­˜ 10 åˆ†é’Ÿ
            cache.set(cache_key, result, expire=600)
            return result
        except Exception as e:
            logger.error(f"AI ç”ŸæˆæŒä»“è¯Šæ–­å¤±è´¥: {e}")
            return {'success': False, 'error': str(e)}
    
    def _build_portfolio_prompt(self, portfolio_data: List[Dict], stats: Dict) -> str:
        """æ„å»ºæŒä»“è¯Šæ–­æç¤ºè¯"""
        prompt = f"""è¯·å¯¹ä»¥ä¸‹ä¸ªäººæŠ•èµ„ç»„åˆè¿›è¡Œæ·±åº¦è¯Šæ–­ï¼š

**ç»„åˆæ±‡æ€»**:
- æŒä»“åŸºé‡‘æ•°: {stats.get('position_count', 0)}
- æ€»æŠ•å…¥æˆæœ¬: {stats.get('total_cost', 0):.2f}
- å½“å‰é¢„ä¼°å¸‚å€¼: {stats.get('total_value', 0):.2f} (è¯¥æ•°æ®ä»…ä¾›å‚è€ƒ)
- å†å²ç›ˆäº: {stats.get('total_profit', 0):.2f} ({stats.get('profit_pct', 0):.2f}%)

**èµ„äº§åˆ†ç±»åˆ†å¸ƒ**:
"""
        for cat, weight in stats.get('category_distribution', {}).items():
            prompt += f"- {cat}: {weight:.1f}%\n"
            
        prompt += "\n**æ ¸å¿ƒè¡Œä¸š/ä¸»é¢˜åˆ†å¸ƒ**:\n"
        for theme, weight in stats.get('theme_distribution', {}).items():
            prompt += f"- {theme}: {weight:.1f}%\n"
            
        prompt += "\n**ä¸ªè‚¡ç©¿é€åˆ†å¸ƒ (å‰10å¤§é‡ä»“è‚¡)**:\n"
        for stock, weight in stats.get('stock_exposure', {}).items():
            # stock_exposure é‡Œçš„æƒé‡æ˜¯ 0-100 çš„ç™¾åˆ†æ¯”
            prompt += f"- {stock}: {weight:.1f}%\n"
            
        prompt += "\n**è¯¦ç»†æŒä»“æ˜ç»†**:\n"
        for i, p in enumerate(portfolio_data, 1):
            prompt += f"{i}. {p.get('fund_name')} ({p.get('fund_code')}): ä»½é¢ {p.get('shares')}, æˆæœ¬ä»· {p.get('cost_price')}\n"
            
        return prompt

    async def generate_recommendation_summary(
        self, 
        funds: List[Dict], 
        theme: str = None,
        news_list: List[Dict] = None
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨èæ‘˜è¦"""
        if not funds:
            return {
                'success': False,
                'error': 'æ— åŸºé‡‘æ•°æ®'
            }
        
        # ç»Ÿè®¡æ•°æ®
        total_funds = len(funds)
        avg_score = sum(f.get('score', 0) for f in funds) / total_funds if total_funds else 0
        high_alpha_count = sum(1 for f in funds if f.get('alpha', 0) > 10)
        low_risk_count = sum(1 for f in funds if f.get('max_drawdown', 0) < 15)
        
        theme_text = theme if theme else "å…¨å¸‚åœº"
        
        # News Analysis (Simple Keyword Based)
        sentiment_text = "å¸‚åœºæƒ…ç»ªå¹³ç¨³"
        sentiment_class = "text-muted"
        hot_topic = "æ— æ˜æ˜¾çƒ­ç‚¹"
        
        if news_list:
            pos_words = ["ä¸Šæ¶¨", "æ–°é«˜", "å¤§æ¶¨", "çªç ´", "åˆ©å¥½", "surge", "record"]
            neg_words = ["ä¸‹è·Œ", "è·³æ°´", "æ–°ä½", "ç ´ä½", "åˆ©ç©º", "crash", "drop"]
            
            pos_count = sum(1 for n in news_list for w in pos_words if w in n['title'])
            neg_count = sum(1 for n in news_list for w in neg_words if w in n['title'])
            
            if pos_count > neg_count * 1.5:
                sentiment_text = "å¤šå¤´æƒ…ç»ªä¸»å¯¼"
                sentiment_class = "highlight-gold"
            elif neg_count > pos_count * 1.5:
                sentiment_text = "é¿é™©æƒ…ç»ªå‡æ¸©"
            
            # Simple topic extraction
            topics = {}
            for n in news_list:
                for k in ["ç§‘æŠ€", "åŒ»è¯", "æ–°èƒ½æº", "åˆ¸å•†", "åŠå¯¼ä½“", "ç¾è‚¡", "æ¸¯è‚¡"]:
                    if k in n['title']:
                        topics[k] = topics.get(k, 0) + 1
            if topics:
                hot_topic = max(topics.items(), key=lambda x: x[1])[0]

        # ç”Ÿæˆç¬¦åˆç”¨æˆ·è¦æ±‚çš„ HTML ç»“æ„ (ä»…ä¿ç•™æ‘˜è¦éƒ¨åˆ†ï¼Œå„æ¨¡å—ç‹¬ç«‹æ¸²æŸ“)
        html_content = f"""
<div class="analysis-intro">
  <p style="margin-bottom: 0.5rem; color: rgba(255,255,255,0.9); font-size: 0.95rem;">
    åŸºäºæœ¬å‘¨æœ€æ–°æ•°æ®å¤ç›˜ï¼Œ<b>{theme_text}</b> å…±ç­›é€‰å‡º <b>{total_funds}</b> åªä¼˜è´¨åŸºé‡‘ã€‚
  </p>
  
  <div class="ai-pulse" style="display: flex; flex-wrap: wrap; gap: 0.5rem; align-items: center;">
    <span class="pulse-tag" style="background: rgba(255,255,255,0.1); padding: 4px 10px; border-radius: 20px; font-size: 0.8rem; display: flex; align-items: center; color: #bdc3c7;">
        ğŸ¤– è¦†ç›– {len(news_list) if news_list else 0} æ¡å¿«è®¯
    </span>
    <span class="pulse-tag {sentiment_class}" style="background: rgba(255,255,255,0.1); padding: 4px 10px; border-radius: 20px; font-size: 0.8rem; display: flex; align-items: center;">
        ğŸ“Š {sentiment_text}
    </span>
    <span class="pulse-tag" style="background: rgba(255,255,255,0.1); padding: 4px 10px; border-radius: 20px; font-size: 0.8rem; display: flex; align-items: center; color: #f1c40f;">
        ğŸ”¥ {hot_topic}
    </span>
  </div>
</div>
"""
        return {
            'success': True,
            'content': html_content,
            'source': 'rule_html'
        }

    async def analyze_fund_news(self, code: str, news_list: List[Dict]) -> str:
        """åˆ†æåŸºé‡‘æ–°é—»ä¸å…¬å‘Š"""
        try:
            if not news_list:
                return "è¿‘æœŸæ— é‡è¦æ–°é—»å…¬å‘Šã€‚"
                
            news_text = "\n".join([f"- {n['date']} [{n.get('type','æ–°é—»')}] {n['title']}" for n in news_list[:5]])
            
            prompt = f"è¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å…¬å‘Šï¼Œåˆ†æåŸºé‡‘ {code} çš„è¿‘æœŸåŠ¨å‘å’Œæ½œåœ¨å½±å“ï¼š\n{news_text}\nè¯·ç®€çŸ­æ€»ç»“ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚"
            system_prompt = "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èèˆ†æƒ…åˆ†æå¸ˆã€‚"
            
            result = await self.ask_ai(prompt, system_prompt=system_prompt, max_tokens=300)
            
            if result.get('success'):
                return result.get('content')
            return "åˆ†æä¸å¯ç”¨"
        except Exception as e:
            logger.error(f"News analysis failed: {e}")
            return f"åˆ†æå¤±è´¥: {e}"


    async def generate_market_macro_analysis(self, news_list: List[Dict]) -> str:
        """ç”Ÿæˆå¸‚åœºå®è§‚ AI åˆ†ææŠ¥å‘Š (åŸºäºæ–°é—»ã€å±€åŠ¿)"""
        try:
            if not news_list:
                return "å½“å‰ç¼ºä¹è¶³å¤Ÿçš„å®æ—¶æ–°é—»æ•°æ®è¿›è¡Œå®è§‚åˆ†æã€‚"

            news_text = "\n".join([f"- {n['date']} {n['title']} ({n.get('source', 'æœªçŸ¥')})" for n in news_list[:10]])
            
            system_prompt = """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„é¦–å¸­ç»æµå­¦å®¶å’ŒæŠ•èµ„ç­–ç•¥å¸ˆã€‚
è¯·æ ¹æ®æä¾›çš„å®æ—¶æ–°é—»ã€æ”¿æ²»å±€åŠ¿å’Œå¸‚åœºçƒ­ç‚¹ï¼Œè¿›è¡Œæ·±åº¦å®è§‚åˆ†æã€‚
ä½ çš„å›å¤å¿…é¡»åŒ…å«ï¼š
1. **è¡Œæƒ…ç»¼è¿°**ï¼šä¸€å¥è¯æ¦‚æ‹¬å½“å‰å¸‚åœºæ ¸å¿ƒé©±åŠ¨åŠ›ã€‚
2. **AI é¢„åˆ¤**ï¼šåŸºäºå½“å‰æ–°é—»ï¼ˆå¦‚é™å‡†ã€ç¾è”å‚¨åŠ æ¯ã€å¤§é€‰ã€æ”¿ç­–å¯¼å‘ç­‰ï¼‰é¢„æµ‹çŸ­æœŸèµ°åŠ¿ã€‚
3. **ç­–ç•¥å»ºè®®**ï¼š
   - âœ… **æ¨èå…³æ³¨**ï¼šå“ªäº›æ¿å—/é£æ ¼ç›®å‰å…·å¤‡æŠ•èµ„ä»·å€¼ï¼Œç®€è¿°ç†ç”±ã€‚
   - âš ï¸ **è§„é¿è­¦å‘Š**ï¼šå“ªäº›æ¿å—/é£æ ¼ç›®å‰å­˜åœ¨è¾ƒé«˜é£é™©ï¼Œæ˜ç¡®è¯´æ˜åŸå› ï¼ˆå³ç”¨æˆ·æåˆ°çš„â€œå“ªäº›ä¸èƒ½ä¹°â€ï¼‰ã€‚
è¯·ä½¿ç”¨ Markdown æ ¼å¼ï¼Œè¯­æ°”ä¸“ä¸šã€æƒå¨ã€æœæ–­ã€‚æ§åˆ¶åœ¨ 300 å­—ä»¥å†…ã€‚"""

            user_prompt = f"ä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¸‚åœºçƒ­ç‚¹æ–°é—»å’Œå…¬å‘Šäº‹é¡¹ï¼š\n\n{news_text}\n\nè¯·ä»¥æ­¤ç”Ÿæˆæœ€æ–°çš„æŠ•èµ„ç­–ç•¥æŒ‡å¯¼ã€‚"
            
            # æ£€æŸ¥ç¼“å­˜
            import hashlib
            cache_key = f"macro_analysis:{hashlib.md5(user_prompt.encode()).hexdigest()}"
            cached = self.db.get_ai_cache(cache_key)
            if cached:
                 return cached

            result = await self.ask_ai(user_prompt, system_prompt=system_prompt, max_tokens=1000)
            
            if result.get('success'):
                content = result.get('content')
                self.db.set_ai_cache(cache_key, content, self.current_model, ttl_hours=2)
                return content
            return "AI æœåŠ¡æš‚æ—¶æ— æ³•å¤„ç†å®è§‚æ•°æ®åˆ†æã€‚"
        except Exception as e:
            logger.error(f"Macro analysis failed: {e}")
            return f"åˆ†æå¤±è´¥: {e}"

    async def generate_structured_fund_analysis(self, fund_name: str, code: str, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»“æ„åŒ–åŸºé‡‘åˆ†æ - éµå¾ª v4.0 JSON åè®®"""
        try:
            # Create a unique cache key based on metrics hash
            metrics_hash = self._generate_metrics_hash(metrics)
            cache_key = f"structured_analysis:{code}:{metrics_hash}"
            
            cached_result = self.db.get_ai_cache(cache_key)
            if cached_result:
                try:
                    return json.loads(cached_result)
                except json.JSONDecodeError:
                    logger.warning(f"Cached structured analysis for {code} is not valid JSON. Re-generating.")
                    # If cached content is invalid, proceed to generate new content

            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŸºé‡‘æŠ•ç ”åŠ©æ‰‹ã€‚ä½ çš„è¾“å‡ºå°†è¢«å‰ç«¯ç›´æ¥æ¸²æŸ“ä¸º UI å¡ç‰‡ï¼Œå¿…é¡»ä¸¥æ ¼éµå®ˆ JSON æ ¼å¼ã€‚
æ¯ä¸€ä¸ªç»“è®ºå¿…é¡»å¸¦æœ‰ [citation] å­—æ®µè¯´æ˜æ•°æ®æ¥æºã€‚è¯­æ°”å®¢è§‚ã€å†·é™ï¼Œç¦æ­¢ä½¿ç”¨è¥é”€è¯æœ¯ã€‚
å¦‚æœæ•°æ®ä¸è¶³ï¼Œç›´æ¥è¾“å‡º nullï¼Œä¸è¦ç¼–é€ ã€‚
"""
            user_prompt = f"""è¯·å¯¹åŸºé‡‘ {fund_name} ({code}) è¿›è¡Œç»“æ„åŒ–æ·±åº¦åˆ†æã€‚
æ•°æ® Context:
- å¤æƒå‡€å€¼æ—¥æœŸ: {metrics.get('nav_date')}
- è¿‘1å¹´æ”¶ç›Šç‡: {metrics.get('return_1y', 0)}%
- æœ€å¤§å›æ’¤: {metrics.get('max_drawdown', 0)}%
- å¤æ™®æ¯”ç‡: {metrics.get('sharpe', 0)}
- Betaå€¼: {metrics.get('beta', 1.0)}

è¯·è¿”å›å¦‚ä¸‹æ ¼å¼çš„ JSON (ä¸è¦åŒ…å« Markdown æ ¼å¼å—):
{{
  "summary_card": {{
    "title": "åŸºè°ƒæ€»ç»“",
    "verdict": "é€‚åˆ...çš„æŠ•èµ„è€…ã€‚",
    "tags": ["...", "..."],
    "citation": "åŸºäº..."
  }},
  "attribution_card": {{
    "title": "è¿‘æœŸè¡¨ç°å½’å› ",
    "points": [
      {{ "reason": "...", "impact": "...", "source": "..." }}
    ]
  }},
  "stress_test": {{
    "scenario": "è‹¥æ²ªæ·±300ä¸‹è·Œ10%",
    "prediction": "é¢„è®¡ä¸‹è·Œ...",
    "beta_ref": "å†å²Betaå€¼ä¸º..."
  }},
  "manager_report": {{
    "style": "ä»·å€¼/æˆé•¿/å‡è¡¡/åšå¼ˆ",
    "rating": "A/B/C",
    "description": "ç²¾ç‚¼çš„ä¸€å¥è¯æè¿°ç»ç†é£æ ¼"
  }}
}}
"""
            result = await self.ask_ai(user_prompt, system_prompt=system_prompt, max_tokens=1000)
            
            if result.get('success'):
                content = result.get('content', '')
                # Clean content
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                elif "```" in content:
                    content = content.split("```")[1].split("```")[0].strip()
                
                try:
                    parsed_content = json.loads(content)
                    self.db.set_ai_cache(cache_key, content, self.current_model, ttl_hours=24) # Cache for 24 hours
                    return parsed_content
                except Exception as e:
                    logger.error(f"JSON Parse Error: {e}\nContent: {content}")
                    return {"error": "è§£æå¤±è´¥", "raw": content}
            
            return {"error": "AI è°ƒç”¨å¤±è´¥"}
        except Exception as e:
            logger.error(f"Structured analysis failed: {e}")
            return {"error": str(e)}

    async def translate_semantic_query(self, query: str) -> Dict[str, Any]:
        """å°†è‡ªç„¶è¯­è¨€æŸ¥è¯¢è½¬æ¢ä¸ºç»“æ„åŒ–è¿‡æ»¤æ¡ä»¶ (Deep Search) + æ™ºèƒ½å¯¹è¯"""
        try:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŸºé‡‘æŠ•èµ„åŠ©æ‰‹ (AI Fund Advisor)ã€‚
ä½ çš„ç›®æ ‡æ˜¯ç†è§£ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€è¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ã€‚

è¯·æ ¹æ®ç”¨æˆ·æ„å›¾é€‰æ‹©ä¸åŒçš„è¿”å›ç­–ç•¥ï¼š

1. **æ„å›¾ï¼šç­›é€‰åŸºé‡‘/æŸ¥æ‰¾æ•°æ®**
   - æå–ç»“æ„åŒ–è¿‡æ»¤æ¡ä»¶åˆ° `filters` å­—æ®µã€‚
   - åœ¨ `interpretation` å­—æ®µä¸­ç”¨è‡ªç„¶è¯­è¨€ï¼ˆä¸­æ–‡ï¼‰æè¿°ä½ çš„ç­›é€‰é€»è¾‘ã€‚
   - å¯ç”¨ç»´åº¦ï¼š
     - themes (åˆ—è¡¨): ['ç§‘æŠ€', 'åŒ»ç–—', 'æ¶ˆè´¹', 'æ–°èƒ½æº', 'äººå·¥æ™ºèƒ½', 'çº¢åˆ©', 'ç™½é…’', 'åŠå¯¼ä½“', 'å†›å·¥', 'ç¨³å¥']
     - return_1y (å¯¹è±¡): { "op": ">"|"<", "val": æ•°å€¼(ç™¾åˆ†æ¯”) }
     - max_drawdown_1y (å¯¹è±¡): { "op": ">"|"<", "val": æ•°å€¼(ç™¾åˆ†æ¯”) }
     - sharpe_1y (å¯¹è±¡): { "op": ">"|"<", "val": æ•°å€¼ }
     - risk_level (å­—ç¬¦ä¸²): 'ä½é£é™©'|'ä¸­é£é™©'|'é«˜é£é™©'

2. **æ„å›¾ï¼šé—²èŠ/é—®å€™/é€šç”¨ç†è´¢é—®é¢˜**
   - `filters` å­—æ®µè®¾ä¸ºç©ºå¯¹è±¡ `{}`ã€‚
   - åœ¨ `interpretation` å­—æ®µä¸­ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œæˆ–è€…è¿›è¡Œç¤¼è²Œçš„å¯¹è¯ã€‚è¯­æ°”è¦ä¸“ä¸šã€äº²åˆ‡ä¸”æœ‰å¸®åŠ©ã€‚
   - å¦‚æœç”¨æˆ·é—®â€œä½ ä¹°ä»€ä¹ˆèƒ½èµšé’±â€ï¼Œè¿™å±äºé€šç”¨ç†è´¢é—®é¢˜ï¼Œä½ å¯ä»¥å»ºè®®ç”¨æˆ·å…³æ³¨â€œé«˜è¯„åˆ†â€æˆ–â€œè¿‘æœŸçƒ­ç‚¹â€åŸºé‡‘ï¼Œå¹¶å°è¯•è½¬æ¢æˆå®½æ¾çš„ç­›é€‰æ¡ä»¶ï¼ˆå¦‚ score > 80ï¼‰ã€‚

**è¾“å‡ºæ ¼å¼ç¤ºä¾‹ (JSON)**:
{
  "filters": { "themes": ["ç§‘æŠ€"], "return_1y": { "op": ">", "val": 10 } },
  "interpretation": "æ˜ç™½äº†ï¼Œæ­£åœ¨ä¸ºæ‚¨ç­›é€‰è¿‘ä¸€å¹´æ”¶ç›Šç‡è¶…è¿‡ 10% çš„ç§‘æŠ€ç±»åŸºé‡‘..."
}

æˆ– (é—²èŠ):
{
  "filters": {},
  "interpretation": "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„ AI ç§äººç®¡å®¶ï¼Œå¯ä»¥å¸®æ‚¨ç­›é€‰åŸºé‡‘ã€è¯Šæ–­æŒä»“æˆ–åˆ†æå¸‚åœºçƒ­ç‚¹ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨ï¼Ÿ"
}

è¯·ç›´æ¥è¿”å› JSONï¼Œä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚"""
            
            user_prompt = f"ç”¨æˆ·è¾“å…¥: \"{query}\"\nè¯·åˆ†ææ„å›¾å¹¶ç”Ÿæˆ JSON å“åº”ã€‚"
            
            result = await self.ask_ai(user_prompt, system_prompt=system_prompt, max_tokens=500)
            
            if result.get('success'):
                content = result.get('content', '{}')
                # Clean block markings if any
                if "```" in content:
                    content = content.split("```")[1]
                    if content.startswith("json"):
                        content = content[4:]
                try:
                    data = json.loads(content.strip())
                    # Flatten logic for backward compatibility
                    parsed = data.get('filters', {})
                    parsed['interpretation'] = data.get('interpretation', 'æ­£åœ¨æœç´¢...')
                    return parsed
                except:
                    logger.error(f"Semantic parse failed: {content}")
                    # Fallback
                    return {"themes": [query], "interpretation": f"æ­£åœ¨ä¸ºæ‚¨æœç´¢åŒ…å« '{query}' çš„åŸºé‡‘..."}
            return {}
        except Exception as e:
            logger.error(f"Semantic translation error: {e}")
            return {}



    async def generate_manager_rating(self, name: str, career_summary: str) -> Dict[str, Any]:
        """ç”Ÿæˆç»ç† AI è¯„çº§ä¸é£æ ¼ç”»åƒ"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            summary_hash = hashlib.md5(career_summary.encode()).hexdigest()
            cache_key = f"manager_rating:{name}:{summary_hash}"
            cached_json = self.db.get_ai_cache(cache_key)
            if cached_json:
                try:
                    return json.loads(cached_json)
                except:
                    pass

            system_prompt = "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åŸºé‡‘è¯„ä»·ä¸“å®¶ï¼Œæ“…é•¿ä»è¿‡å¾€ä¸šç»©å’ŒæŠ•æ•™è¨€è®ºä¸­æ€»ç»“ç»ç†é£æ ¼ã€‚"
            user_prompt = f"è¯·æ ¹æ®åŸºé‡‘ç»ç† {name} çš„å±¥å†æ€»ç»“ï¼Œç»™å‡º AI é£æ ¼æ ‡ç­¾å’Œèƒ½åŠ›è¯„çº§ï¼š\n{career_summary}\nè¯·è¿”å› JSON: {{'style': '...', 'rating': 'A/B/C', 'pros': ['...', '...']}}"
            
            result = await self.ask_ai(user_prompt, system_prompt=system_prompt, max_tokens=500)
            if result.get('success'):
                content = result.get('content', '{}')
                if "```" in content:
                    content = content.split("```")[1]
                    if content.startswith("json"): content = content[4:]
                
                content = content.strip()
                try:
                     parsed = json.loads(content)
                     self.db.set_ai_cache(cache_key, content, self.current_model, ttl_hours=24*7) # ç»ç†é£æ ¼é•¿æœŸä¸å˜
                     return parsed
                except:
                     return {"style": "æ•°æ®è§£æé”™è¯¯", "rating": "C", "pros": []}

            return {"style": "å‡è¡¡å‹", "rating": "B", "pros": ["æ•°æ®ä¸è¶³"]}
        except Exception as e:
            logger.error(f"Manager rating failed: {e}")
            return {"style": "æœªçŸ¥", "rating": "B", "pros": []}

# å…¨å±€å•ä¾‹
_ai_service: Optional[AIService] = None

def get_ai_service() -> Optional[AIService]:
    """è·å– AI æœåŠ¡å®ä¾‹"""
    global _ai_service
    
    settings = get_settings()
    if not settings.AI_API_KEY:
        return None
    
    if _ai_service is None:
        _ai_service = AIService()
    
    return _ai_service
